{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexLinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ComplexLinearLayer, self).__init__()\n",
    "        self.W_r = Linear(in_features, out_features,bias=True) \n",
    "        self.W_i = Linear(in_features, out_features,bias=True)\n",
    "        \n",
    "    def forward(self,in_r, in_i): # in = x \n",
    "        Real_OUT = self.W_r(in_r) - self.W_i(in_i)\n",
    "        Imag_OUT = self.W_r(in_i) + self.W_i(in_r)\n",
    "        return Real_OUT, Imag_OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_sign_activation(in_r, in_i): \n",
    "    return torch.sign(in_r), torch.sign(in_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNetworkPaper(nn.Module):\n",
    "    def __init__(self,in_features,hidden_features):\n",
    "        super(ComplexNetworkPaper,self).__init__()\n",
    "        self.input_linear_Layer = ComplexLinearLayer(in_features,hidden_features)\n",
    "        \n",
    "    def forward(self,in_r,in_i):\n",
    "        hidden_r,hidden_i = self.input_linear_Layer(in_r,in_i)\n",
    "        output_r,output_i = complex_sign_activation(hidden_r,hidden_i)\n",
    "        return output_r,output_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_with_tanh(input_r, input_i, target_r, target_i, H_r,H_i, bias_r,bias_i, lam=0.01):\n",
    "    '''\n",
    "    bias_r: z for the real part\n",
    "    bias_i: z for the imaginary part\n",
    "    H_r: weight matrix( the channel matrix) (real)\n",
    "    H_i: weight matrix( the channel matrix) (imaginary)\n",
    "    input_r: real part of the input\n",
    "    input_i: imaginary part of the input\n",
    "    target_r: real part of the target\n",
    "    target_i: imaginary part of the target\n",
    "    '''\n",
    "    # Compute H * x_n + z for r and i parts\n",
    "    temp_r = torch.matmul(input_r, H_r.t()) + bias_r\n",
    "    temp_i = torch.matmul(input_i, H_i.t()) + bias_i\n",
    "    \n",
    "    # (tanh(Hx_n + z))\n",
    "    tanh_r = torch.tanh(temp_r)\n",
    "    tanh_i = torch.tanh(temp_i)\n",
    "    \n",
    "    # first_term = 1/N sum [tanh(H * x_n + z)(REAL) + tanh(H * x_n + z)(IMAG)]\n",
    "    first_term = torch.mean(torch.norm(target_r - tanh_r, dim=1)**2 + torch.norm(target_i - tanh_i, dim=1)**2)\n",
    "    \n",
    "\n",
    "    regul_term = lam * (torch.norm(H_r, p=2)+torch.norm(H_i, p=2))\n",
    "    loss = first_term + regul_term\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_r = torch.randn(10, 5)  # 10 is the batch size, so the forwad function will handle these 10 samples at once, and so the same weights are being used for all of these 10 samples \n",
    "in_i = torch.randn(10, 5)\n",
    "target_r = torch.randn(10, 5) \n",
    "target_i = torch.randn(10, 5)\n",
    "model = ComplexNetworkPaper(in_features=in_r.shape[1], hidden_features=target_r.shape[1])\n",
    "optimizer = SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 10.04552936553955\n",
      "Epoch 1, Loss: 10.027202606201172\n",
      "Epoch 2, Loss: 10.008940696716309\n",
      "Epoch 3, Loss: 9.990747451782227\n",
      "Epoch 4, Loss: 9.97261905670166\n",
      "Epoch 5, Loss: 9.954554557800293\n",
      "Epoch 6, Loss: 9.936559677124023\n",
      "Epoch 7, Loss: 9.918630599975586\n",
      "Epoch 8, Loss: 9.900768280029297\n",
      "Epoch 9, Loss: 9.88297176361084\n",
      "Epoch 10, Loss: 9.865241050720215\n",
      "Epoch 11, Loss: 9.847578048706055\n",
      "Epoch 12, Loss: 9.829981803894043\n",
      "Epoch 13, Loss: 9.812451362609863\n",
      "Epoch 14, Loss: 9.79498291015625\n",
      "Epoch 15, Loss: 9.777586936950684\n",
      "Epoch 16, Loss: 9.760254859924316\n",
      "Epoch 17, Loss: 9.742988586425781\n",
      "Epoch 18, Loss: 9.725787162780762\n",
      "Epoch 19, Loss: 9.708650588989258\n",
      "Epoch 20, Loss: 9.691583633422852\n",
      "Epoch 21, Loss: 9.674577713012695\n",
      "Epoch 22, Loss: 9.657639503479004\n",
      "Epoch 23, Loss: 9.640765190124512\n",
      "Epoch 24, Loss: 9.623956680297852\n",
      "Epoch 25, Loss: 9.607213973999023\n",
      "Epoch 26, Loss: 9.590533256530762\n",
      "Epoch 27, Loss: 9.573920249938965\n",
      "Epoch 28, Loss: 9.557369232177734\n",
      "Epoch 29, Loss: 9.540884971618652\n",
      "Epoch 30, Loss: 9.52446174621582\n",
      "Epoch 31, Loss: 9.508102416992188\n",
      "Epoch 32, Loss: 9.49180793762207\n",
      "Epoch 33, Loss: 9.475576400756836\n",
      "Epoch 34, Loss: 9.459407806396484\n",
      "Epoch 35, Loss: 9.443302154541016\n",
      "Epoch 36, Loss: 9.427257537841797\n",
      "Epoch 37, Loss: 9.411276817321777\n",
      "Epoch 38, Loss: 9.395358085632324\n",
      "Epoch 39, Loss: 9.379500389099121\n",
      "Epoch 40, Loss: 9.363704681396484\n",
      "Epoch 41, Loss: 9.347970962524414\n",
      "Epoch 42, Loss: 9.332297325134277\n",
      "Epoch 43, Loss: 9.316685676574707\n",
      "Epoch 44, Loss: 9.30113410949707\n",
      "Epoch 45, Loss: 9.285642623901367\n",
      "Epoch 46, Loss: 9.270211219787598\n",
      "Epoch 47, Loss: 9.254839897155762\n",
      "Epoch 48, Loss: 9.239527702331543\n",
      "Epoch 49, Loss: 9.224274635314941\n",
      "Epoch 50, Loss: 9.209078788757324\n",
      "Epoch 51, Loss: 9.193944931030273\n",
      "Epoch 52, Loss: 9.17886734008789\n",
      "Epoch 53, Loss: 9.163848876953125\n",
      "Epoch 54, Loss: 9.148886680603027\n",
      "Epoch 55, Loss: 9.13398265838623\n",
      "Epoch 56, Loss: 9.119135856628418\n",
      "Epoch 57, Loss: 9.104344367980957\n",
      "Epoch 58, Loss: 9.089612007141113\n",
      "Epoch 59, Loss: 9.074934959411621\n",
      "Epoch 60, Loss: 9.06031322479248\n",
      "Epoch 61, Loss: 9.045748710632324\n",
      "Epoch 62, Loss: 9.031238555908203\n",
      "Epoch 63, Loss: 9.016783714294434\n",
      "Epoch 64, Loss: 9.0023832321167\n",
      "Epoch 65, Loss: 8.988038063049316\n",
      "Epoch 66, Loss: 8.973747253417969\n",
      "Epoch 67, Loss: 8.95950984954834\n",
      "Epoch 68, Loss: 8.945326805114746\n",
      "Epoch 69, Loss: 8.931195259094238\n",
      "Epoch 70, Loss: 8.91711711883545\n",
      "Epoch 71, Loss: 8.903093338012695\n",
      "Epoch 72, Loss: 8.889121055603027\n",
      "Epoch 73, Loss: 8.875200271606445\n",
      "Epoch 74, Loss: 8.861331939697266\n",
      "Epoch 75, Loss: 8.847516059875488\n",
      "Epoch 76, Loss: 8.83375072479248\n",
      "Epoch 77, Loss: 8.820035934448242\n",
      "Epoch 78, Loss: 8.806371688842773\n",
      "Epoch 79, Loss: 8.792760848999023\n",
      "Epoch 80, Loss: 8.779197692871094\n",
      "Epoch 81, Loss: 8.7656831741333\n",
      "Epoch 82, Loss: 8.75222110748291\n",
      "Epoch 83, Loss: 8.73880672454834\n",
      "Epoch 84, Loss: 8.725441932678223\n",
      "Epoch 85, Loss: 8.712124824523926\n",
      "Epoch 86, Loss: 8.698856353759766\n",
      "Epoch 87, Loss: 8.685637474060059\n",
      "Epoch 88, Loss: 8.672465324401855\n",
      "Epoch 89, Loss: 8.659343719482422\n",
      "Epoch 90, Loss: 8.646265983581543\n",
      "Epoch 91, Loss: 8.633235931396484\n",
      "Epoch 92, Loss: 8.620255470275879\n",
      "Epoch 93, Loss: 8.607318878173828\n",
      "Epoch 94, Loss: 8.594429016113281\n",
      "Epoch 95, Loss: 8.581586837768555\n",
      "Epoch 96, Loss: 8.568787574768066\n",
      "Epoch 97, Loss: 8.556036949157715\n",
      "Epoch 98, Loss: 8.543330192565918\n",
      "Epoch 99, Loss: 8.530671119689941\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train() # eval vs train \n",
    "    \n",
    "    # Forward pass\n",
    "    output_r, output_i = model(in_r, in_i)\n",
    "    \n",
    "    # LOss\n",
    "    loss = custom_loss_with_tanh(input_r=in_r, input_i=in_i, target_r=target_r, target_i=target_i, H_r=model.input_linear_Layer.W_r.weight,H_i = model.input_linear_Layer.W_i.weight, bias_r=model.input_linear_Layer.W_r.bias,bias_i=model.input_linear_Layer.W_i.bias)\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()  # reset the gradients\n",
    "    loss.backward()  # Backpropgation to calculation the new gradients\n",
    "    optimizer.step()  # Update the weights in the direction of these new gradients\n",
    "\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3109,  0.3562, -0.1505,  0.1037, -0.1914],\n",
      "        [ 0.2683, -0.3811,  0.1777,  0.1989, -0.3673],\n",
      "        [-0.4057,  0.1451,  0.1896, -0.3522,  0.2909],\n",
      "        [-0.3734, -0.4211, -0.0590,  0.0278, -0.4328],\n",
      "        [ 0.2413,  0.0966,  0.2457,  0.0462, -0.0449]])\n",
      "tensor([ 0.2005, -0.1995, -0.4651, -0.1121, -0.3915])\n",
      "tensor([[ 0.2729,  0.0279, -0.0104,  0.2520,  0.2719],\n",
      "        [ 0.4346, -0.4275,  0.3036,  0.3758, -0.3768],\n",
      "        [ 0.3558,  0.3291,  0.0900, -0.1054, -0.0410],\n",
      "        [ 0.1467, -0.3146, -0.0909, -0.2807, -0.4620],\n",
      "        [-0.2189, -0.0509,  0.3681,  0.4110,  0.3457]])\n",
      "tensor([-0.3976, -0.0818, -0.2187, -0.1184, -0.0827])\n"
     ]
    }
   ],
   "source": [
    "H = {name: param for name, param in model.named_parameters()}\n",
    "for name, param in model.named_parameters():\n",
    "    H[name] = param.data\n",
    "for h in H:\n",
    "    print(H[h])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
