{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Linear\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexLinearLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(ComplexLinearLayer, self).__init__()\n",
    "        self.W_r = Linear(in_features, out_features,bias=True)\n",
    "        self.W_i = Linear(in_features, out_features,bias=True)\n",
    "        \n",
    "    def forward(self,in_r, in_i):\n",
    "        Real_OUT = self.W_r(in_r) - self.W_i(in_i)\n",
    "        Imag_OUT = self.W_r(in_i) + self.W_i(in_r)\n",
    "        return Real_OUT, Imag_OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complex_sign_activation(in_r, in_i):\n",
    "    return torch.sign(in_r), torch.sign(in_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplexNetworkPaper(nn.Module):\n",
    "    def __init__(self,in_features,hidden_features):\n",
    "        super(ComplexNetworkPaper,self).__init__()\n",
    "        self.input_linear_Layer = ComplexLinearLayer(in_features,hidden_features)\n",
    "        \n",
    "    def forward(self,in_r,in_i):\n",
    "        hidden_r,hidden_i = self.input_linear_Layer(in_r,in_i)\n",
    "        output_r,output_i = complex_sign_activation(hidden_r,hidden_i)\n",
    "        \n",
    "        return output_r,output_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_with_tanh(input_r, input_i, target_r, target_i, H_r,H_i, bias_r,bias_i, lam=0.01):\n",
    "    '''\n",
    "    bias_r: z for the real part\n",
    "    bias_i: z for the imaginary part\n",
    "    H_r: weight matrix( the channel matrix) (real)\n",
    "    H_i: weight matrix( the channel matrix) (imaginary)\n",
    "    input_r: real part of the input\n",
    "    input_i: imaginary part of the input\n",
    "    target_r: real part of the target\n",
    "    target_i: imaginary part of the target\n",
    "    '''\n",
    "    # Compute H * x_n + z for r and i parts\n",
    "    temp_r = torch.matmul(input_r, H_r.t()) + bias_r\n",
    "    temp_i = torch.matmul(input_i, H_i.t()) + bias_i\n",
    "    \n",
    "    # (tanh(Hx_n + z))\n",
    "    tanh_r = torch.tanh(temp_r)\n",
    "    tanh_i = torch.tanh(temp_i)\n",
    "    \n",
    "    # first_term = 1/N sum [tanh(H * x_n + z)(REAL) + tanh(H * x_n + z)(IMAG)]\n",
    "    first_term = torch.mean(torch.norm(target_r - tanh_r, dim=1)**2 + torch.norm(target_i - tanh_i, dim=1)**2)\n",
    "    \n",
    "\n",
    "    regul_term = lam * (torch.norm(H_r, p=2)+torch.norm(H_i, p=2))\n",
    "    loss = first_term + regul_term\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_r = torch.randn(10, 5)  # 10 is the batch size, so the forwad function will handle these 10 samples at once, and so the same weights are being used for all of these 10 samples \n",
    "in_i = torch.randn(10, 5)\n",
    "target_r = torch.randn(10, 5)  \n",
    "target_i = torch.randn(10, 5)\n",
    "model = ComplexNetworkPaper(in_features=in_r.shape[1], hidden_features=target_r.shape[1])\n",
    "optimizer = SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 13.144113540649414\n",
      "Epoch 1, Loss: 13.131444931030273\n",
      "Epoch 2, Loss: 13.118783950805664\n",
      "Epoch 3, Loss: 13.106131553649902\n",
      "Epoch 4, Loss: 13.093485832214355\n",
      "Epoch 5, Loss: 13.080851554870605\n",
      "Epoch 6, Loss: 13.068221092224121\n",
      "Epoch 7, Loss: 13.055602073669434\n",
      "Epoch 8, Loss: 13.042989730834961\n",
      "Epoch 9, Loss: 13.03038501739502\n",
      "Epoch 10, Loss: 13.017786026000977\n",
      "Epoch 11, Loss: 13.005196571350098\n",
      "Epoch 12, Loss: 12.992612838745117\n",
      "Epoch 13, Loss: 12.980036735534668\n",
      "Epoch 14, Loss: 12.96746826171875\n",
      "Epoch 15, Loss: 12.954906463623047\n",
      "Epoch 16, Loss: 12.942350387573242\n",
      "Epoch 17, Loss: 12.929800033569336\n",
      "Epoch 18, Loss: 12.917258262634277\n",
      "Epoch 19, Loss: 12.9047212600708\n",
      "Epoch 20, Loss: 12.892189025878906\n",
      "Epoch 21, Loss: 12.879666328430176\n",
      "Epoch 22, Loss: 12.867146492004395\n",
      "Epoch 23, Loss: 12.854634284973145\n",
      "Epoch 24, Loss: 12.84212589263916\n",
      "Epoch 25, Loss: 12.829625129699707\n",
      "Epoch 26, Loss: 12.81712818145752\n",
      "Epoch 27, Loss: 12.804636001586914\n",
      "Epoch 28, Loss: 12.79215145111084\n",
      "Epoch 29, Loss: 12.779670715332031\n",
      "Epoch 30, Loss: 12.767194747924805\n",
      "Epoch 31, Loss: 12.75472354888916\n",
      "Epoch 32, Loss: 12.742258071899414\n",
      "Epoch 33, Loss: 12.729795455932617\n",
      "Epoch 34, Loss: 12.717337608337402\n",
      "Epoch 35, Loss: 12.704886436462402\n",
      "Epoch 36, Loss: 12.692437171936035\n",
      "Epoch 37, Loss: 12.67999267578125\n",
      "Epoch 38, Loss: 12.667552947998047\n",
      "Epoch 39, Loss: 12.655115127563477\n",
      "Epoch 40, Loss: 12.642684936523438\n",
      "Epoch 41, Loss: 12.630255699157715\n",
      "Epoch 42, Loss: 12.617830276489258\n",
      "Epoch 43, Loss: 12.605409622192383\n",
      "Epoch 44, Loss: 12.592991828918457\n",
      "Epoch 45, Loss: 12.580578804016113\n",
      "Epoch 46, Loss: 12.568169593811035\n",
      "Epoch 47, Loss: 12.55576229095459\n",
      "Epoch 48, Loss: 12.54335880279541\n",
      "Epoch 49, Loss: 12.53095817565918\n",
      "Epoch 50, Loss: 12.518560409545898\n",
      "Epoch 51, Loss: 12.506165504455566\n",
      "Epoch 52, Loss: 12.4937744140625\n",
      "Epoch 53, Loss: 12.4813871383667\n",
      "Epoch 54, Loss: 12.469000816345215\n",
      "Epoch 55, Loss: 12.456619262695312\n",
      "Epoch 56, Loss: 12.44424057006836\n",
      "Epoch 57, Loss: 12.431865692138672\n",
      "Epoch 58, Loss: 12.419490814208984\n",
      "Epoch 59, Loss: 12.407119750976562\n",
      "Epoch 60, Loss: 12.394752502441406\n",
      "Epoch 61, Loss: 12.382389068603516\n",
      "Epoch 62, Loss: 12.370025634765625\n",
      "Epoch 63, Loss: 12.357666015625\n",
      "Epoch 64, Loss: 12.34531021118164\n",
      "Epoch 65, Loss: 12.33295726776123\n",
      "Epoch 66, Loss: 12.320606231689453\n",
      "Epoch 67, Loss: 12.308259010314941\n",
      "Epoch 68, Loss: 12.295915603637695\n",
      "Epoch 69, Loss: 12.283571243286133\n",
      "Epoch 70, Loss: 12.271235466003418\n",
      "Epoch 71, Loss: 12.258898735046387\n",
      "Epoch 72, Loss: 12.246566772460938\n",
      "Epoch 73, Loss: 12.234237670898438\n",
      "Epoch 74, Loss: 12.221911430358887\n",
      "Epoch 75, Loss: 12.209590911865234\n",
      "Epoch 76, Loss: 12.197269439697266\n",
      "Epoch 77, Loss: 12.184954643249512\n",
      "Epoch 78, Loss: 12.17264175415039\n",
      "Epoch 79, Loss: 12.160332679748535\n",
      "Epoch 80, Loss: 12.148030281066895\n",
      "Epoch 81, Loss: 12.135727882385254\n",
      "Epoch 82, Loss: 12.123431205749512\n",
      "Epoch 83, Loss: 12.111138343811035\n",
      "Epoch 84, Loss: 12.098851203918457\n",
      "Epoch 85, Loss: 12.086567878723145\n",
      "Epoch 86, Loss: 12.074289321899414\n",
      "Epoch 87, Loss: 12.062015533447266\n",
      "Epoch 88, Loss: 12.0497465133667\n",
      "Epoch 89, Loss: 12.037483215332031\n",
      "Epoch 90, Loss: 12.025224685668945\n",
      "Epoch 91, Loss: 12.012970924377441\n",
      "Epoch 92, Loss: 12.000725746154785\n",
      "Epoch 93, Loss: 11.988483428955078\n",
      "Epoch 94, Loss: 11.976249694824219\n",
      "Epoch 95, Loss: 11.964020729064941\n",
      "Epoch 96, Loss: 11.951797485351562\n",
      "Epoch 97, Loss: 11.939583778381348\n",
      "Epoch 98, Loss: 11.927374839782715\n",
      "Epoch 99, Loss: 11.915176391601562\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # Forward pass\n",
    "    output_r, output_i = model(in_r, in_i)\n",
    "    \n",
    "    # LOss\n",
    "    loss = custom_loss_with_tanh(input_r=in_r, input_i=in_i, target_r=target_r, target_i=target_i, H_r=model.input_linear_Layer.W_r.weight,H_i = model.input_linear_Layer.W_i.weight, bias_r=model.input_linear_Layer.W_r.bias,bias_i=model.input_linear_Layer.W_i.bias)\n",
    "    \n",
    "    # Backpropagation and optimization\n",
    "    optimizer.zero_grad()  # reset the gradients\n",
    "    loss.backward()  # Backpropgation to calculation the new gradients\n",
    "    optimizer.step()  # Update the weights in the direction of these new gradients\n",
    "\n",
    "     print(f'Epoch {epoch}, Loss: {loss.item()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
